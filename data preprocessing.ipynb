{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Missing Values: Check for missing data in all columns. Impute numerical columns (e.g., Host_Popularity_percentage, Guest_Popularity_percentage, Listening_Time_minutes) with median or mean. For categorical columns (e.g., Podcast_Name, Genre), use mode or a placeholder like \"Unknown.\"\n",
    "\n",
    "Remove Duplicates: Drop duplicate rows based on id to ensure unique episodes.\n",
    "\n",
    "Data Type Conversion: Ensure correct data types (e.g., Episode_Length_minutes as float, Publication_Day as categorical, Publication_Time as datetime).\n",
    "\n",
    "Outlier Detection: Identify and cap outliers in numerical columns (e.g., Listening_Time_minutes, Number_of_Ads) using IQR or z-score methods.\n",
    "\n",
    "Text Cleaning: Standardize text in Podcast_Name and Episode_Title (lowercase, remove special characters) for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           750000 non-null  int64  \n",
      " 1   Podcast_Name                 750000 non-null  object \n",
      " 2   Episode_Title                750000 non-null  object \n",
      " 3   Episode_Length_minutes       662907 non-null  float64\n",
      " 4   Genre                        750000 non-null  object \n",
      " 5   Host_Popularity_percentage   750000 non-null  float64\n",
      " 6   Publication_Day              750000 non-null  object \n",
      " 7   Publication_Time             750000 non-null  object \n",
      " 8   Guest_Popularity_percentage  603970 non-null  float64\n",
      " 9   Number_of_Ads                749999 non-null  float64\n",
      " 10  Episode_Sentiment            750000 non-null  object \n",
      " 11  Listening_Time_minutes       750000 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 68.7+ MB\n",
      "Dataset info:\n",
      " None\n",
      "Publication_Time sample values:\n",
      " 0        Night\n",
      "1    Afternoon\n",
      "2      Evening\n",
      "3      Morning\n",
      "4    Afternoon\n",
      "5    Afternoon\n",
      "6        Night\n",
      "7        Night\n",
      "8      Evening\n",
      "9        Night\n",
      "Name: Publication_Time, dtype: object\n",
      "Publication_Time unique values:\n",
      " ['Night' 'Afternoon' 'Evening' 'Morning']\n",
      "Missing values after imputation:\n",
      " id                             0\n",
      "Podcast_Name                   0\n",
      "Episode_Title                  0\n",
      "Episode_Length_minutes         0\n",
      "Genre                          0\n",
      "Host_Popularity_percentage     0\n",
      "Publication_Day                0\n",
      "Publication_Time               0\n",
      "Guest_Popularity_percentage    0\n",
      "Number_of_Ads                  0\n",
      "Episode_Sentiment              0\n",
      "Listening_Time_minutes         0\n",
      "dtype: int64\n",
      "Shape after removing duplicates: (750000, 12)\n",
      "Stats after outlier capping:\n",
      "        Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count           750000.000000               750000.000000   \n",
      "mean                64.427335                   59.859901   \n",
      "std                 30.995758                   22.873098   \n",
      "min                  0.000000                    1.300000   \n",
      "25%                 39.420000                   39.410000   \n",
      "50%                 63.840000                   60.050000   \n",
      "75%                 90.310000                   79.530000   \n",
      "max                166.645000                  119.460000   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
      "count                750000.000000  750000.000000           750000.000000  \n",
      "mean                     52.498047       1.347928               45.437406  \n",
      "std                      25.537152       1.111032               27.138306  \n",
      "min                       0.000000       0.000000                0.000000  \n",
      "25%                      34.550000       0.000000               23.178350  \n",
      "50%                      53.580000       1.000000               43.379460  \n",
      "75%                      71.040000       2.000000               64.811580  \n",
      "max                     119.910000       5.000000              119.970000  \n",
      "Preprocessed data saved as 'preprocessed_podcast_data.csv'\n",
      "Final columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Listening_Time_minutes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Load the Dataset\n",
    "# Replace 'dataset.csv' with your file path\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Display basic info to understand structure\n",
    "print(\"Dataset info:\\n\", df.info())\n",
    "print(\"Publication_Time sample values:\\n\", df['Publication_Time'].head(10))\n",
    "print(\"Publication_Time unique values:\\n\", df['Publication_Time'].unique())\n",
    "\n",
    "# 2. Handle Missing Values\n",
    "# Numerical columns\n",
    "num_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "            'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "# Categorical columns (now including Publication_Time)\n",
    "cat_cols = ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', \n",
    "            'Episode_Sentiment', 'Publication_Time']\n",
    "imputer_cat = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after imputation:\\n\", df.isnull().sum())\n",
    "\n",
    "# 3. Remove Duplicates\n",
    "# Drop duplicates based on 'id'\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "print(\"Shape after removing duplicates:\", df.shape)\n",
    "\n",
    "# 4. Data Type Conversion\n",
    "# Numerical columns\n",
    "df['Episode_Length_minutes'] = df['Episode_Length_minutes'].astype(float)\n",
    "df['Host_Popularity_percentage'] = df['Host_Popularity_percentage'].astype(float)\n",
    "df['Guest_Popularity_percentage'] = df['Guest_Popularity_percentage'].astype(float)\n",
    "df['Number_of_Ads'] = df['Number_of_Ads'].astype(int)\n",
    "df['Listening_Time_minutes'] = df['Listening_Time_minutes'].astype(float)\n",
    "\n",
    "# Categorical columns\n",
    "df['Publication_Day'] = df['Publication_Day'].astype('category')\n",
    "df['Genre'] = df['Genre'].astype('category')\n",
    "df['Episode_Sentiment'] = df['Episode_Sentiment'].astype('category')\n",
    "df['Publication_Time'] = df['Publication_Time'].astype('category')\n",
    "\n",
    "# 5. Outlier Detection\n",
    "# Cap outliers using IQR for numerical columns\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Verify outlier handling\n",
    "print(\"Stats after outlier capping:\\n\", df[num_cols].describe())\n",
    "\n",
    "# 6. Text Cleaning\n",
    "# Standardize text columns: lowercase, remove special characters\n",
    "df['Podcast_Name'] = df['Podcast_Name'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "df['Episode_Title'] = df['Episode_Title'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Save preprocessed dataset\n",
    "df.to_csv('preprocessed_podcast_data.csv', index=False)\n",
    "print(\"Preprocessed data saved as 'preprocessed_podcast_data.csv'\")\n",
    "print(\"Final columns:\", df.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
