{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         750000\n",
       "1         750001\n",
       "2         750002\n",
       "3         750003\n",
       "4         750004\n",
       "           ...  \n",
       "249995    999995\n",
       "249996    999996\n",
       "249997    999997\n",
       "249998    999998\n",
       "249999    999999\n",
       "Name: id, Length: 250000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "ids = test_df['id'].copy()\n",
    "\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment']\n",
      "Test data shape: (250000, 11)\n",
      "'id' column found in test data.\n",
      "Training data columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Listening_Time_minutes']\n",
      "Loaded Linear Regression model.\n",
      "Predictions saved as 'predictions.csv'\n",
      "Output shape: (250000, 2)\n",
      "Sample predictions:\n",
      "        id  Listening_Time_minutes\n",
      "0  750000                     0.0\n",
      "1  750001                     0.0\n",
      "2  750002                     0.0\n",
      "3  750003                     0.0\n",
      "4  750004                     0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# 1. Load the Test Data\n",
    "# Replace 'test_data.csv' with your test file path\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(\"Test data columns:\", test_df.columns.tolist())\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Check for 'id' column\n",
    "if 'id' not in test_df.columns:\n",
    "    print(\"Warning: 'id' column not found in test data. Generating synthetic IDs.\")\n",
    "    test_df['id'] = range(1, len(test_df) + 1)\n",
    "else:\n",
    "    print(\"'id' column found in test data.\")\n",
    "\n",
    "# Load training data for encoding and scaling\n",
    "train_df = pd.read_csv('preprocessed_podcast_data.csv')\n",
    "print(\"Training data columns:\", train_df.columns.tolist())\n",
    "\n",
    "# 2. Preprocess the Test Data\n",
    "# Handle Missing Values\n",
    "num_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "            'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "cat_cols = ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', \n",
    "            'Episode_Sentiment', 'Publication_Time']\n",
    "\n",
    "# Numerical imputation\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "test_df[num_cols] = imputer_num.fit_transform(test_df[num_cols])\n",
    "train_df[num_cols] = imputer_num.fit_transform(train_df[num_cols])\n",
    "\n",
    "# Categorical imputation\n",
    "imputer_cat = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "test_df[cat_cols] = imputer_cat.fit_transform(test_df[cat_cols])\n",
    "train_df[cat_cols] = imputer_cat.fit_transform(train_df[cat_cols])\n",
    "\n",
    "# Data Type Conversion\n",
    "for df in [train_df, test_df]:\n",
    "    df['Episode_Length_minutes'] = df['Episode_Length_minutes'].astype(float)\n",
    "    df['Host_Popularity_percentage'] = df['Host_Popularity_percentage'].astype(float)\n",
    "    df['Guest_Popularity_percentage'] = df['Guest_Popularity_percentage'].astype(float)\n",
    "    df['Number_of_Ads'] = df['Number_of_Ads'].astype(int)\n",
    "    df['Publication_Day'] = df['Publication_Day'].astype('category')\n",
    "    df['Genre'] = df['Genre'].astype('category')\n",
    "    df['Episode_Sentiment'] = df['Episode_Sentiment'].astype('category')\n",
    "    df['Publication_Time'] = df['Publication_Time'].astype('category')\n",
    "\n",
    "# Outlier Capping\n",
    "for df in [train_df, test_df]:\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Text Cleaning\n",
    "for df in [train_df, test_df]:\n",
    "    df['Podcast_Name'] = df['Podcast_Name'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "    df['Episode_Title'] = df['Episode_Title'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "# Categorical Encoding\n",
    "cat_cols = ['Genre', 'Publication_Day', 'Episode_Sentiment', 'Publication_Time']\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(train_df[cat_cols])  # Fit on training data\n",
    "for df in [train_df, test_df]:\n",
    "    ohe_features = ohe.transform(df[cat_cols])\n",
    "    ohe_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "    df_ohe = pd.DataFrame(ohe_features, columns=ohe_feature_names, index=df.index)\n",
    "    df.drop(cat_cols, axis=1, inplace=True)\n",
    "    df = pd.concat([df, df_ohe], axis=1)\n",
    "\n",
    "# Target encode Podcast_Name\n",
    "podcast_mean_listening = train_df.groupby('Podcast_Name')['Listening_Time_minutes'].mean()\n",
    "for df in [train_df, test_df]:\n",
    "    df['Podcast_Name_Encoded'] = df['Podcast_Name'].map(podcast_mean_listening)\n",
    "    df['Podcast_Name_Encoded'] = df['Podcast_Name_Encoded'].fillna(podcast_mean_listening.mean())\n",
    "    df.drop('Podcast_Name', axis=1, inplace=True)\n",
    "\n",
    "# Text Features from Episode_Title\n",
    "vectorizer = CountVectorizer(max_features=50, stop_words='english')\n",
    "vectorizer.fit(train_df['Episode_Title'])  # Fit on training data\n",
    "for df in [train_df, test_df]:\n",
    "    df['Title_Length'] = df['Episode_Title'].apply(lambda x: len(x.split()))\n",
    "    keywords = ['interview', 'exclusive', 'special', 'guest']\n",
    "    for kw in keywords:\n",
    "        df[f'Title_Has_{kw}'] = df['Episode_Title'].str.contains(kw, case=False, na=False).astype(int)\n",
    "    title_bow = vectorizer.transform(df['Episode_Title'])\n",
    "    bow_df = pd.DataFrame(title_bow.toarray(), \n",
    "                          columns=[f'Title_BOW_{f}' for f in vectorizer.get_feature_names_out()],\n",
    "                          index=df.index)\n",
    "    df.drop('Episode_Title', axis=1, inplace=True)\n",
    "    df = pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "# Is_Weekend from encoded Publication_Day\n",
    "for df in [train_df, test_df]:\n",
    "    weekend_cols = [col for col in df.columns if 'Publication_Day_Saturday' in col or 'Publication_Day_Sunday' in col]\n",
    "    df['Is_Weekend'] = df[weekend_cols].sum(axis=1).astype(int) if weekend_cols else 0\n",
    "\n",
    "# Interaction Features\n",
    "for df in [train_df, test_df]:\n",
    "    df['Combined_Popularity'] = (df['Host_Popularity_percentage'] + df['Guest_Popularity_percentage']) / 2\n",
    "    df['Ad_Density'] = df['Number_of_Ads'] / df['Episode_Length_minutes'].replace(0, 1)\n",
    "\n",
    "# Scaling Numerical Features\n",
    "num_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "            'Guest_Popularity_percentage', 'Number_of_Ads', \n",
    "            'Podcast_Name_Encoded', 'Title_Length', \n",
    "            'Combined_Popularity', 'Ad_Density']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[num_cols])  # Fit on engineered training data\n",
    "train_df[num_cols] = scaler.transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "# Ensure test data has same columns as training data\n",
    "engineered_train_df = pd.read_csv('engineered_podcast_data.csv')\n",
    "train_cols = engineered_train_df.drop('Listening_Time_minutes', axis=1).columns\n",
    "test_cols = test_df.columns\n",
    "missing_cols = [col for col in train_cols if col not in test_cols and col != 'id']\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  # Add missing columns with zeros\n",
    "extra_cols = [col for col in test_cols if col not in train_cols and col != 'id']\n",
    "test_df = test_df.drop(extra_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reorder columns to match training data\n",
    "test_df = test_df[train_cols]\n",
    "\n",
    "# 4. Load the Model\n",
    "model = joblib.load('LinearRegression_model.pkl')\n",
    "print(\"Loaded Linear Regression model.\")\n",
    "\n",
    "# 5. Make Predictions\n",
    "test_df['id'] = ids\n",
    "# Store 'id' column separately to avoid dropping issues\n",
    "test_ids = test_df['id']\n",
    "test_features = test_df.drop('id', axis=1)\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# 6. Save Output\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Listening_Time_minutes': predictions\n",
    "})\n",
    "output_df['Listening_Time_minutes'] = output_df['Listening_Time_minutes'].clip(lower=0)  # Ensure non-negative predictions\n",
    "output_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions saved as 'predictions.csv'\")\n",
    "print(\"Output shape:\", output_df.shape)\n",
    "print(\"Sample predictions:\\n\", output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Genre_Business</th>\n",
       "      <th>Genre_Comedy</th>\n",
       "      <th>Genre_Education</th>\n",
       "      <th>Genre_Health</th>\n",
       "      <th>Genre_Lifestyle</th>\n",
       "      <th>Genre_Music</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_BOW_84</th>\n",
       "      <th>Title_BOW_85</th>\n",
       "      <th>Title_BOW_86</th>\n",
       "      <th>Title_BOW_87</th>\n",
       "      <th>Title_BOW_88</th>\n",
       "      <th>Title_BOW_99</th>\n",
       "      <th>Title_BOW_episode</th>\n",
       "      <th>Is_Weekend</th>\n",
       "      <th>Combined_Popularity</th>\n",
       "      <th>Ad_Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.468860</td>\n",
       "      <td>-0.950895</td>\n",
       "      <td>0.032578</td>\n",
       "      <td>-0.313158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.604057</td>\n",
       "      <td>-0.400594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.179431</td>\n",
       "      <td>0.499718</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>-1.213222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354963</td>\n",
       "      <td>-0.646622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150752</td>\n",
       "      <td>0.351072</td>\n",
       "      <td>1.762608</td>\n",
       "      <td>-1.213222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531719</td>\n",
       "      <td>-0.646622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.644183</td>\n",
       "      <td>-1.594009</td>\n",
       "      <td>-0.029293</td>\n",
       "      <td>0.586907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.074471</td>\n",
       "      <td>-0.309914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254637</td>\n",
       "      <td>-0.076942</td>\n",
       "      <td>-1.613260</td>\n",
       "      <td>0.586907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.240516</td>\n",
       "      <td>-0.109388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>-1.399461</td>\n",
       "      <td>0.258387</td>\n",
       "      <td>1.719142</td>\n",
       "      <td>1.486971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.438445</td>\n",
       "      <td>2.121984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>0.679857</td>\n",
       "      <td>-0.803997</td>\n",
       "      <td>-0.860631</td>\n",
       "      <td>0.586907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.165724</td>\n",
       "      <td>-0.192204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>-1.687888</td>\n",
       "      <td>-1.483836</td>\n",
       "      <td>0.829848</td>\n",
       "      <td>-0.313158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368128</td>\n",
       "      <td>0.957538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>1.581916</td>\n",
       "      <td>-0.716558</td>\n",
       "      <td>1.609106</td>\n",
       "      <td>1.486971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713332</td>\n",
       "      <td>-0.132969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>0.439501</td>\n",
       "      <td>-0.793068</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>-1.213222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.573736</td>\n",
       "      <td>-0.646622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Episode_Length_minutes  Host_Popularity_percentage  \\\n",
       "0                     0.468860                   -0.950895   \n",
       "1                    -1.179431                    0.499718   \n",
       "2                     0.150752                    0.351072   \n",
       "3                     1.644183                   -1.594009   \n",
       "4                     0.254637                   -0.076942   \n",
       "...                        ...                         ...   \n",
       "249995               -1.399461                    0.258387   \n",
       "249996                0.679857                   -0.803997   \n",
       "249997               -1.687888                   -1.483836   \n",
       "249998                1.581916                   -0.716558   \n",
       "249999                0.439501                   -0.793068   \n",
       "\n",
       "        Guest_Popularity_percentage  Number_of_Ads  Genre_Business  \\\n",
       "0                          0.032578      -0.313158               0   \n",
       "1                          0.033753      -1.213222               0   \n",
       "2                          1.762608      -1.213222               0   \n",
       "3                         -0.029293       0.586907               0   \n",
       "4                         -1.613260       0.586907               0   \n",
       "...                             ...            ...             ...   \n",
       "249995                     1.719142       1.486971               0   \n",
       "249996                    -0.860631       0.586907               0   \n",
       "249997                     0.829848      -0.313158               0   \n",
       "249998                     1.609106       1.486971               0   \n",
       "249999                    -0.067668      -1.213222               0   \n",
       "\n",
       "        Genre_Comedy  Genre_Education  Genre_Health  Genre_Lifestyle  \\\n",
       "0                  0                0             0                0   \n",
       "1                  0                0             0                0   \n",
       "2                  0                0             0                0   \n",
       "3                  0                0             0                0   \n",
       "4                  0                0             0                0   \n",
       "...              ...              ...           ...              ...   \n",
       "249995             0                0             0                0   \n",
       "249996             0                0             0                0   \n",
       "249997             0                0             0                0   \n",
       "249998             0                0             0                0   \n",
       "249999             0                0             0                0   \n",
       "\n",
       "        Genre_Music  ...  Title_BOW_84  Title_BOW_85  Title_BOW_86  \\\n",
       "0                 0  ...             0             0             0   \n",
       "1                 0  ...             0             0             0   \n",
       "2                 0  ...             0             0             0   \n",
       "3                 0  ...             0             0             0   \n",
       "4                 0  ...             0             0             0   \n",
       "...             ...  ...           ...           ...           ...   \n",
       "249995            0  ...             0             0             0   \n",
       "249996            0  ...             0             0             0   \n",
       "249997            0  ...             0             0             0   \n",
       "249998            0  ...             0             0             0   \n",
       "249999            0  ...             0             0             0   \n",
       "\n",
       "        Title_BOW_87  Title_BOW_88  Title_BOW_99  Title_BOW_episode  \\\n",
       "0                  0             0             0                  0   \n",
       "1                  0             0             0                  0   \n",
       "2                  0             0             0                  0   \n",
       "3                  0             0             0                  0   \n",
       "4                  0             0             0                  0   \n",
       "...              ...           ...           ...                ...   \n",
       "249995             0             0             0                  0   \n",
       "249996             0             0             0                  0   \n",
       "249997             0             0             0                  0   \n",
       "249998             0             0             0                  0   \n",
       "249999             0             0             0                  0   \n",
       "\n",
       "        Is_Weekend  Combined_Popularity  Ad_Density  \n",
       "0                0            -0.604057   -0.400594  \n",
       "1                0             0.354963   -0.646622  \n",
       "2                0             1.531719   -0.646622  \n",
       "3                0            -1.074471   -0.309914  \n",
       "4                0            -1.240516   -0.109388  \n",
       "...            ...                  ...         ...  \n",
       "249995           0             1.438445    2.121984  \n",
       "249996           0            -1.165724   -0.192204  \n",
       "249997           0            -0.368128    0.957538  \n",
       "249998           0             0.713332   -0.132969  \n",
       "249999           0            -0.573736   -0.646622  \n",
       "\n",
       "[250000 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data feature stats:\n",
      "        Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count           250000.000000               250000.000000   \n",
      "mean                 0.001758                   -0.006270   \n",
      "std                  1.000240                    1.000304   \n",
      "min                 -1.998899                   -2.508184   \n",
      "25%                 -0.808412                   -0.901055   \n",
      "50%                 -0.014755                    0.001753   \n",
      "75%                  0.841492                    0.853846   \n",
      "max                  3.316348                    2.531364   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  Genre_Business  \\\n",
      "count                250000.000000  250000.000000        250000.0   \n",
      "mean                     -0.003026      -0.000580             0.0   \n",
      "std                       0.999344       1.001514             0.0   \n",
      "min                      -2.055753      -1.213222             0.0   \n",
      "25%                      -0.702821      -1.213222             0.0   \n",
      "50%                       0.033753      -0.313158             0.0   \n",
      "75%                       0.724120       0.586907             0.0   \n",
      "max                       2.518761       3.287100             0.0   \n",
      "\n",
      "       Genre_Comedy  Genre_Education  Genre_Health  Genre_Lifestyle  \\\n",
      "count      250000.0         250000.0      250000.0         250000.0   \n",
      "mean            0.0              0.0           0.0              0.0   \n",
      "std             0.0              0.0           0.0              0.0   \n",
      "min             0.0              0.0           0.0              0.0   \n",
      "25%             0.0              0.0           0.0              0.0   \n",
      "50%             0.0              0.0           0.0              0.0   \n",
      "75%             0.0              0.0           0.0              0.0   \n",
      "max             0.0              0.0           0.0              0.0   \n",
      "\n",
      "       Genre_Music  ...  Title_BOW_85  Title_BOW_86  Title_BOW_87  \\\n",
      "count     250000.0  ...      250000.0      250000.0      250000.0   \n",
      "mean           0.0  ...           0.0           0.0           0.0   \n",
      "std            0.0  ...           0.0           0.0           0.0   \n",
      "min            0.0  ...           0.0           0.0           0.0   \n",
      "25%            0.0  ...           0.0           0.0           0.0   \n",
      "50%            0.0  ...           0.0           0.0           0.0   \n",
      "75%            0.0  ...           0.0           0.0           0.0   \n",
      "max            0.0  ...           0.0           0.0           0.0   \n",
      "\n",
      "       Title_BOW_88  Title_BOW_99  Title_BOW_episode  Is_Weekend  \\\n",
      "count      250000.0      250000.0           250000.0    250000.0   \n",
      "mean            0.0           0.0                0.0         0.0   \n",
      "std             0.0           0.0                0.0         0.0   \n",
      "min             0.0           0.0                0.0         0.0   \n",
      "25%             0.0           0.0                0.0         0.0   \n",
      "50%             0.0           0.0                0.0         0.0   \n",
      "75%             0.0           0.0                0.0         0.0   \n",
      "max             0.0           0.0                0.0         0.0   \n",
      "\n",
      "       Combined_Popularity     Ad_Density             id  \n",
      "count        250000.000000  250000.000000  250000.000000  \n",
      "mean             -0.006372      -0.003786  874999.500000  \n",
      "std               0.999571       0.994814   72168.927986  \n",
      "min              -3.023699      -0.646622  750000.000000  \n",
      "25%              -0.726498      -0.646622  812499.750000  \n",
      "50%               0.009011      -0.278228  874999.500000  \n",
      "75%               0.712755       0.138917  937499.250000  \n",
      "max               2.677577      15.816417  999999.000000  \n",
      "\n",
      "[8 rows x 88 columns]\n",
      "Missing values in test data:\n",
      " Episode_Length_minutes         0\n",
      "Host_Popularity_percentage     0\n",
      "Guest_Popularity_percentage    0\n",
      "Number_of_Ads                  0\n",
      "Genre_Business                 0\n",
      "                              ..\n",
      "Title_BOW_episode              0\n",
      "Is_Weekend                     0\n",
      "Combined_Popularity            0\n",
      "Ad_Density                     0\n",
      "id                             0\n",
      "Length: 88, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data feature stats:\\n\", test_df.describe())\n",
    "print(\"Missing values in test data:\\n\", test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients:\n",
      " [ 2.38420138e+01 -3.24718189e+11 -3.62538473e+11 -2.62222893e+00\n",
      "  7.96946757e+11  7.96946757e+11  7.96946757e+11  7.96946757e+11\n",
      "  7.96946757e+11  7.96946757e+11  7.96946757e+11  7.96946757e+11\n",
      "  7.96946757e+11  7.96946757e+11  1.98072601e+12  1.98072601e+12\n",
      "  2.84126992e+12  2.84126992e+12  1.98072601e+12  1.98072601e+12\n",
      "  1.98072601e+12  1.73778657e+12  1.73778657e+12  1.73778657e+12\n",
      "  1.37492269e+12  1.37492269e+12  1.37492269e+12  1.37492269e+12\n",
      "  3.22267391e-01  5.43287701e+12 -2.95850070e+12 -3.31142494e+12\n",
      " -3.93009261e+12  3.88447662e+12 -4.22363281e-01 -4.52026367e-01\n",
      " -1.41357422e-01  7.10449219e-01 -4.02832031e-01  2.52929688e-01\n",
      "  4.00390625e-01 -2.38708496e-01 -3.15917969e-01  5.82763672e-01\n",
      "  6.26098633e-01  1.51464844e+00  7.36816406e-01  5.28564453e-01\n",
      "  4.61181641e-01  7.30957031e-01  8.52050781e-02 -5.66528320e-01\n",
      "  1.19628906e-02 -1.90490723e-01 -5.68115234e-01 -3.47412109e-01\n",
      "  6.51626587e-02 -6.42578125e-01  4.96093750e-01  2.08496094e-01\n",
      "  8.96423340e-01 -6.18408203e-01 -1.22851562e+00 -6.93115234e-01\n",
      "  5.53710938e-01  4.86083984e-01  3.82690430e-02  7.27661133e-01\n",
      "  7.70874023e-01 -4.16503906e-01 -5.61401367e-01 -2.21466064e-01\n",
      " -8.30566406e-01  3.13476562e-01  5.13916016e-02  5.78613281e-02\n",
      " -3.03039551e-01 -1.11816406e-01 -1.80175781e-01 -1.79443359e-01\n",
      " -2.39257812e-01 -9.45434570e-02  3.13949585e-01  0.00000000e+00\n",
      " -8.60543904e+11  4.91612595e+11  1.09277344e+00]\n",
      "Model intercept: -5890382028820.951\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('LinearRegression_model.pkl')\n",
    "print(\"Model coefficients:\\n\", model.coef_)\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training columns: ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre_Business', 'Genre_Comedy', 'Genre_Education', 'Genre_Health', 'Genre_Lifestyle', 'Genre_Music', 'Genre_News', 'Genre_Sports', 'Genre_Technology', 'Genre_True Crime', 'Publication_Day_Friday', 'Publication_Day_Monday', 'Publication_Day_Saturday', 'Publication_Day_Sunday', 'Publication_Day_Thursday', 'Publication_Day_Tuesday', 'Publication_Day_Wednesday', 'Episode_Sentiment_Negative', 'Episode_Sentiment_Neutral', 'Episode_Sentiment_Positive', 'Publication_Time_Afternoon', 'Publication_Time_Evening', 'Publication_Time_Morning', 'Publication_Time_Night', 'Podcast_Name_Encoded', 'Title_Length', 'Title_Has_interview', 'Title_Has_exclusive', 'Title_Has_special', 'Title_Has_guest', 'Title_BOW_12', 'Title_BOW_18', 'Title_BOW_19', 'Title_BOW_20', 'Title_BOW_23', 'Title_BOW_24', 'Title_BOW_26', 'Title_BOW_27', 'Title_BOW_28', 'Title_BOW_29', 'Title_BOW_30', 'Title_BOW_31', 'Title_BOW_32', 'Title_BOW_33', 'Title_BOW_34', 'Title_BOW_35', 'Title_BOW_42', 'Title_BOW_43', 'Title_BOW_46', 'Title_BOW_47', 'Title_BOW_48', 'Title_BOW_49', 'Title_BOW_51', 'Title_BOW_52', 'Title_BOW_54', 'Title_BOW_58', 'Title_BOW_61', 'Title_BOW_62', 'Title_BOW_63', 'Title_BOW_64', 'Title_BOW_65', 'Title_BOW_67', 'Title_BOW_69', 'Title_BOW_70', 'Title_BOW_71', 'Title_BOW_72', 'Title_BOW_73', 'Title_BOW_78', 'Title_BOW_79', 'Title_BOW_80', 'Title_BOW_81', 'Title_BOW_82', 'Title_BOW_83', 'Title_BOW_84', 'Title_BOW_85', 'Title_BOW_86', 'Title_BOW_87', 'Title_BOW_88', 'Title_BOW_99', 'Title_BOW_episode', 'Is_Weekend', 'Combined_Popularity', 'Ad_Density']\n",
      "Test columns: ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre_Business', 'Genre_Comedy', 'Genre_Education', 'Genre_Health', 'Genre_Lifestyle', 'Genre_Music', 'Genre_News', 'Genre_Sports', 'Genre_Technology', 'Genre_True Crime', 'Publication_Day_Friday', 'Publication_Day_Monday', 'Publication_Day_Saturday', 'Publication_Day_Sunday', 'Publication_Day_Thursday', 'Publication_Day_Tuesday', 'Publication_Day_Wednesday', 'Episode_Sentiment_Negative', 'Episode_Sentiment_Neutral', 'Episode_Sentiment_Positive', 'Publication_Time_Afternoon', 'Publication_Time_Evening', 'Publication_Time_Morning', 'Publication_Time_Night', 'Podcast_Name_Encoded', 'Title_Length', 'Title_Has_interview', 'Title_Has_exclusive', 'Title_Has_special', 'Title_Has_guest', 'Title_BOW_12', 'Title_BOW_18', 'Title_BOW_19', 'Title_BOW_20', 'Title_BOW_23', 'Title_BOW_24', 'Title_BOW_26', 'Title_BOW_27', 'Title_BOW_28', 'Title_BOW_29', 'Title_BOW_30', 'Title_BOW_31', 'Title_BOW_32', 'Title_BOW_33', 'Title_BOW_34', 'Title_BOW_35', 'Title_BOW_42', 'Title_BOW_43', 'Title_BOW_46', 'Title_BOW_47', 'Title_BOW_48', 'Title_BOW_49', 'Title_BOW_51', 'Title_BOW_52', 'Title_BOW_54', 'Title_BOW_58', 'Title_BOW_61', 'Title_BOW_62', 'Title_BOW_63', 'Title_BOW_64', 'Title_BOW_65', 'Title_BOW_67', 'Title_BOW_69', 'Title_BOW_70', 'Title_BOW_71', 'Title_BOW_72', 'Title_BOW_73', 'Title_BOW_78', 'Title_BOW_79', 'Title_BOW_80', 'Title_BOW_81', 'Title_BOW_82', 'Title_BOW_83', 'Title_BOW_84', 'Title_BOW_85', 'Title_BOW_86', 'Title_BOW_87', 'Title_BOW_88', 'Title_BOW_99', 'Title_BOW_episode', 'Is_Weekend', 'Combined_Popularity', 'Ad_Density', 'id']\n"
     ]
    }
   ],
   "source": [
    "engineered_train_df = pd.read_csv('engineered_podcast_data.csv')\n",
    "train_cols = engineered_train_df.drop('Listening_Time_minutes', axis=1).columns\n",
    "print(\"Training columns:\", train_cols.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment']\n",
      "Test data shape: (250000, 11)\n",
      "Test data sample:\n",
      "        id         Podcast_Name Episode_Title  Episode_Length_minutes  \\\n",
      "0  750000  Educational Nuggets    Episode 73                   78.96   \n",
      "1  750001          Sound Waves    Episode 23                   27.87   \n",
      "2  750002        Joke Junction    Episode 11                   69.10   \n",
      "3  750003        Comedy Corner    Episode 73                  115.39   \n",
      "4  750004         Life Lessons    Episode 50                   72.32   \n",
      "\n",
      "       Genre  Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
      "0  Education                       38.11        Saturday          Evening   \n",
      "1      Music                       71.29          Sunday          Morning   \n",
      "2     Comedy                       67.89          Friday          Evening   \n",
      "3     Comedy                       23.40          Sunday          Morning   \n",
      "4  Lifestyle                       58.10       Wednesday          Morning   \n",
      "\n",
      "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \n",
      "0                        53.33            1.0           Neutral  \n",
      "1                          NaN            0.0           Neutral  \n",
      "2                        97.51            0.0          Positive  \n",
      "3                        51.75            2.0          Positive  \n",
      "4                        11.30            2.0           Neutral  \n",
      "Test data stats:\n",
      "                   id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count  250000.000000            2.212640e+05               250000.000000   \n",
      "mean   874999.500000            4.192987e+02                   59.716491   \n",
      "std     72168.927986            1.668545e+05                   22.880028   \n",
      "min    750000.000000            2.470000e+00                    2.490000   \n",
      "25%    812499.750000            3.578000e+01                   39.250000   \n",
      "50%    874999.500000            6.397000e+01                   59.900000   \n",
      "75%    937499.250000            9.415000e+01                   79.390000   \n",
      "max    999999.000000            7.848626e+07                  117.760000   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  \n",
      "count                201168.000000  250000.000000  \n",
      "mean                     52.192796       1.355852  \n",
      "std                      28.445034       4.274399  \n",
      "min                       0.000000       0.000000  \n",
      "25%                      28.320000       0.000000  \n",
      "50%                      53.360000       1.000000  \n",
      "75%                      76.560000       2.000000  \n",
      "max                     116.820000    2063.000000  \n",
      "'id' column found in test data.\n",
      "Training data columns: ['id', 'Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Listening_Time_minutes']\n",
      "Test data feature stats after engineering:\n",
      "        Episode_Length_minutes  Host_Popularity_percentage  \\\n",
      "count           250000.000000               250000.000000   \n",
      "mean                 0.001758                   -0.006270   \n",
      "std                  1.000240                    1.000304   \n",
      "min                 -1.998899                   -2.508184   \n",
      "25%                 -0.808412                   -0.901055   \n",
      "50%                 -0.014755                    0.001753   \n",
      "75%                  0.841492                    0.853846   \n",
      "max                  3.316348                    2.531364   \n",
      "\n",
      "       Guest_Popularity_percentage  Number_of_Ads  Podcast_Name_Encoded  \\\n",
      "count                250000.000000  250000.000000         250000.000000   \n",
      "mean                     -0.003026      -0.000580             -0.001157   \n",
      "std                       0.999344       1.001514              1.001236   \n",
      "min                      -2.055753      -1.213222             -2.612711   \n",
      "25%                      -0.702821      -1.213222             -0.633845   \n",
      "50%                       0.033753      -0.313158              0.098526   \n",
      "75%                       0.724120       0.586907              0.688325   \n",
      "max                       2.518761       3.287100              1.916686   \n",
      "\n",
      "       Title_Length  Title_Has_interview  Title_Has_exclusive  \\\n",
      "count      250000.0             250000.0             250000.0   \n",
      "mean            0.0                  0.0                  0.0   \n",
      "std             0.0                  0.0                  0.0   \n",
      "min             0.0                  0.0                  0.0   \n",
      "25%             0.0                  0.0                  0.0   \n",
      "50%             0.0                  0.0                  0.0   \n",
      "75%             0.0                  0.0                  0.0   \n",
      "max             0.0                  0.0                  0.0   \n",
      "\n",
      "       Title_Has_special  Title_Has_guest  Is_Weekend  Combined_Popularity  \\\n",
      "count           250000.0         250000.0    250000.0        250000.000000   \n",
      "mean                 0.0              0.0         0.0            -0.006372   \n",
      "std                  0.0              0.0         0.0             0.999571   \n",
      "min                  0.0              0.0         0.0            -3.023699   \n",
      "25%                  0.0              0.0         0.0            -0.726498   \n",
      "50%                  0.0              0.0         0.0             0.009011   \n",
      "75%                  0.0              0.0         0.0             0.712755   \n",
      "max                  0.0              0.0         0.0             2.677577   \n",
      "\n",
      "          Ad_Density  \n",
      "count  250000.000000  \n",
      "mean       -0.003786  \n",
      "std         0.994814  \n",
      "min        -0.646622  \n",
      "25%        -0.646622  \n",
      "50%        -0.278228  \n",
      "75%         0.138917  \n",
      "max        15.816417  \n",
      "Missing values in test data:\n",
      " id                             0\n",
      "Episode_Length_minutes         0\n",
      "Host_Popularity_percentage     0\n",
      "Guest_Popularity_percentage    0\n",
      "Number_of_Ads                  0\n",
      "Podcast_Name_Encoded           0\n",
      "Title_Length                   0\n",
      "Title_Has_interview            0\n",
      "Title_Has_exclusive            0\n",
      "Title_Has_special              0\n",
      "Title_Has_guest                0\n",
      "Is_Weekend                     0\n",
      "Combined_Popularity            0\n",
      "Ad_Density                     0\n",
      "dtype: int64\n",
      "Training columns: ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre_Business', 'Genre_Comedy', 'Genre_Education', 'Genre_Health', 'Genre_Lifestyle', 'Genre_Music', 'Genre_News', 'Genre_Sports', 'Genre_Technology', 'Genre_True Crime', 'Publication_Day_Friday', 'Publication_Day_Monday', 'Publication_Day_Saturday', 'Publication_Day_Sunday', 'Publication_Day_Thursday', 'Publication_Day_Tuesday', 'Publication_Day_Wednesday', 'Episode_Sentiment_Negative', 'Episode_Sentiment_Neutral', 'Episode_Sentiment_Positive', 'Publication_Time_Afternoon', 'Publication_Time_Evening', 'Publication_Time_Morning', 'Publication_Time_Night', 'Podcast_Name_Encoded', 'Title_Length', 'Title_Has_interview', 'Title_Has_exclusive', 'Title_Has_special', 'Title_Has_guest', 'Title_BOW_12', 'Title_BOW_18', 'Title_BOW_19', 'Title_BOW_20', 'Title_BOW_23', 'Title_BOW_24', 'Title_BOW_26', 'Title_BOW_27', 'Title_BOW_28', 'Title_BOW_29', 'Title_BOW_30', 'Title_BOW_31', 'Title_BOW_32', 'Title_BOW_33', 'Title_BOW_34', 'Title_BOW_35', 'Title_BOW_42', 'Title_BOW_43', 'Title_BOW_46', 'Title_BOW_47', 'Title_BOW_48', 'Title_BOW_49', 'Title_BOW_51', 'Title_BOW_52', 'Title_BOW_54', 'Title_BOW_58', 'Title_BOW_61', 'Title_BOW_62', 'Title_BOW_63', 'Title_BOW_64', 'Title_BOW_65', 'Title_BOW_67', 'Title_BOW_69', 'Title_BOW_70', 'Title_BOW_71', 'Title_BOW_72', 'Title_BOW_73', 'Title_BOW_78', 'Title_BOW_79', 'Title_BOW_80', 'Title_BOW_81', 'Title_BOW_82', 'Title_BOW_83', 'Title_BOW_84', 'Title_BOW_85', 'Title_BOW_86', 'Title_BOW_87', 'Title_BOW_88', 'Title_BOW_99', 'Title_BOW_episode', 'Is_Weekend', 'Combined_Popularity', 'Ad_Density']\n",
      "Test columns: ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre_Business', 'Genre_Comedy', 'Genre_Education', 'Genre_Health', 'Genre_Lifestyle', 'Genre_Music', 'Genre_News', 'Genre_Sports', 'Genre_Technology', 'Genre_True Crime', 'Publication_Day_Friday', 'Publication_Day_Monday', 'Publication_Day_Saturday', 'Publication_Day_Sunday', 'Publication_Day_Thursday', 'Publication_Day_Tuesday', 'Publication_Day_Wednesday', 'Episode_Sentiment_Negative', 'Episode_Sentiment_Neutral', 'Episode_Sentiment_Positive', 'Publication_Time_Afternoon', 'Publication_Time_Evening', 'Publication_Time_Morning', 'Publication_Time_Night', 'Podcast_Name_Encoded', 'Title_Length', 'Title_Has_interview', 'Title_Has_exclusive', 'Title_Has_special', 'Title_Has_guest', 'Title_BOW_12', 'Title_BOW_18', 'Title_BOW_19', 'Title_BOW_20', 'Title_BOW_23', 'Title_BOW_24', 'Title_BOW_26', 'Title_BOW_27', 'Title_BOW_28', 'Title_BOW_29', 'Title_BOW_30', 'Title_BOW_31', 'Title_BOW_32', 'Title_BOW_33', 'Title_BOW_34', 'Title_BOW_35', 'Title_BOW_42', 'Title_BOW_43', 'Title_BOW_46', 'Title_BOW_47', 'Title_BOW_48', 'Title_BOW_49', 'Title_BOW_51', 'Title_BOW_52', 'Title_BOW_54', 'Title_BOW_58', 'Title_BOW_61', 'Title_BOW_62', 'Title_BOW_63', 'Title_BOW_64', 'Title_BOW_65', 'Title_BOW_67', 'Title_BOW_69', 'Title_BOW_70', 'Title_BOW_71', 'Title_BOW_72', 'Title_BOW_73', 'Title_BOW_78', 'Title_BOW_79', 'Title_BOW_80', 'Title_BOW_81', 'Title_BOW_82', 'Title_BOW_83', 'Title_BOW_84', 'Title_BOW_85', 'Title_BOW_86', 'Title_BOW_87', 'Title_BOW_88', 'Title_BOW_99', 'Title_BOW_episode', 'Is_Weekend', 'Combined_Popularity', 'Ad_Density']\n",
      "Loaded Linear Regression model.\n",
      "Model coefficients:\n",
      " [ 2.38420138e+01 -3.24718189e+11 -3.62538473e+11 -2.62222893e+00\n",
      "  7.96946757e+11  7.96946757e+11  7.96946757e+11  7.96946757e+11\n",
      "  7.96946757e+11  7.96946757e+11  7.96946757e+11  7.96946757e+11\n",
      "  7.96946757e+11  7.96946757e+11  1.98072601e+12  1.98072601e+12\n",
      "  2.84126992e+12  2.84126992e+12  1.98072601e+12  1.98072601e+12\n",
      "  1.98072601e+12  1.73778657e+12  1.73778657e+12  1.73778657e+12\n",
      "  1.37492269e+12  1.37492269e+12  1.37492269e+12  1.37492269e+12\n",
      "  3.22267391e-01  5.43287701e+12 -2.95850070e+12 -3.31142494e+12\n",
      " -3.93009261e+12  3.88447662e+12 -4.22363281e-01 -4.52026367e-01\n",
      " -1.41357422e-01  7.10449219e-01 -4.02832031e-01  2.52929688e-01\n",
      "  4.00390625e-01 -2.38708496e-01 -3.15917969e-01  5.82763672e-01\n",
      "  6.26098633e-01  1.51464844e+00  7.36816406e-01  5.28564453e-01\n",
      "  4.61181641e-01  7.30957031e-01  8.52050781e-02 -5.66528320e-01\n",
      "  1.19628906e-02 -1.90490723e-01 -5.68115234e-01 -3.47412109e-01\n",
      "  6.51626587e-02 -6.42578125e-01  4.96093750e-01  2.08496094e-01\n",
      "  8.96423340e-01 -6.18408203e-01 -1.22851562e+00 -6.93115234e-01\n",
      "  5.53710938e-01  4.86083984e-01  3.82690430e-02  7.27661133e-01\n",
      "  7.70874023e-01 -4.16503906e-01 -5.61401367e-01 -2.21466064e-01\n",
      " -8.30566406e-01  3.13476562e-01  5.13916016e-02  5.78613281e-02\n",
      " -3.03039551e-01 -1.11816406e-01 -1.80175781e-01 -1.79443359e-01\n",
      " -2.39257812e-01 -9.45434570e-02  3.13949585e-01  0.00000000e+00\n",
      " -8.60543904e+11  4.91612595e+11  1.09277344e+00]\n",
      "Model intercept: -5890382028820.951\n",
      "Prediction stats:\n",
      " count    2.500000e+05\n",
      "mean    -5.890382e+12\n",
      "std      2.360040e+01\n",
      "min     -5.890382e+12\n",
      "25%     -5.890382e+12\n",
      "50%     -5.890382e+12\n",
      "75%     -5.890382e+12\n",
      "max     -5.890382e+12\n",
      "dtype: float64\n",
      "Predictions saved as 'predictions.csv'\n",
      "Output shape: (250000, 2)\n",
      "Sample predictions:\n",
      "        id  Listening_Time_minutes\n",
      "0  750000                     0.0\n",
      "1  750001                     0.0\n",
      "2  750002                     0.0\n",
      "3  750003                     0.0\n",
      "4  750004                     0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# 1. Load the Test Data\n",
    "# Replace 'test_data.csv' with your test file path\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(\"Test data columns:\", test_df.columns.tolist())\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"Test data sample:\\n\", test_df.head())\n",
    "print(\"Test data stats:\\n\", test_df.describe())\n",
    "\n",
    "# Check for 'id' column\n",
    "if 'id' not in test_df.columns:\n",
    "    print(\"Warning: 'id' column not found in test data. Generating synthetic IDs.\")\n",
    "    test_df['id'] = range(1, len(test_df) + 1)\n",
    "else:\n",
    "    print(\"'id' column found in test data.\")\n",
    "\n",
    "# Load training data for encoding and scaling\n",
    "train_df = pd.read_csv('preprocessed_podcast_data.csv')\n",
    "print(\"Training data columns:\", train_df.columns.tolist())\n",
    "\n",
    "# 2. Preprocess the Test Data\n",
    "# Handle Missing Values\n",
    "num_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "            'Guest_Popularity_percentage', 'Number_of_Ads']\n",
    "cat_cols = ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', \n",
    "            'Episode_Sentiment', 'Publication_Time']\n",
    "\n",
    "# Numerical imputation\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "test_df[num_cols] = imputer_num.fit_transform(test_df[num_cols])\n",
    "train_df[num_cols] = imputer_num.fit_transform(train_df[num_cols])\n",
    "\n",
    "# Categorical imputation\n",
    "imputer_cat = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "test_df[cat_cols] = imputer_cat.fit_transform(test_df[cat_cols])\n",
    "train_df[cat_cols] = imputer_cat.fit_transform(train_df[cat_cols])\n",
    "\n",
    "# Data Type Conversion\n",
    "for df in [train_df, test_df]:\n",
    "    df['Episode_Length_minutes'] = df['Episode_Length_minutes'].astype(float)\n",
    "    df['Host_Popularity_percentage'] = df['Host_Popularity_percentage'].astype(float)\n",
    "    df['Guest_Popularity_percentage'] = df['Guest_Popularity_percentage'].astype(float)\n",
    "    df['Number_of_Ads'] = df['Number_of_Ads'].astype(int)\n",
    "    df['Publication_Day'] = df['Publication_Day'].astype('category')\n",
    "    df['Genre'] = df['Genre'].astype('category')\n",
    "    df['Episode_Sentiment'] = df['Episode_Sentiment'].astype('category')\n",
    "    df['Publication_Time'] = df['Publication_Time'].astype('category')\n",
    "\n",
    "# Outlier Capping\n",
    "for df in [train_df, test_df]:\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Text Cleaning\n",
    "for df in [train_df, test_df]:\n",
    "    df['Podcast_Name'] = df['Podcast_Name'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "    df['Episode_Title'] = df['Episode_Title'].str.lower().str.replace(r'[^a-z0-9\\s]', '', regex=True)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "# Categorical Encoding\n",
    "cat_cols = ['Genre', 'Publication_Day', 'Episode_Sentiment', 'Publication_Time']\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(train_df[cat_cols])  # Fit on training data\n",
    "for df in [train_df, test_df]:\n",
    "    ohe_features = ohe.transform(df[cat_cols])\n",
    "    ohe_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "    df_ohe = pd.DataFrame(ohe_features, columns=ohe_feature_names, index=df.index)\n",
    "    df.drop(cat_cols, axis=1, inplace=True)\n",
    "    df = pd.concat([df, df_ohe], axis=1)\n",
    "\n",
    "# Target encode Podcast_Name\n",
    "podcast_mean_listening = train_df.groupby('Podcast_Name')['Listening_Time_minutes'].mean()\n",
    "for df in [train_df, test_df]:\n",
    "    df['Podcast_Name_Encoded'] = df['Podcast_Name'].map(podcast_mean_listening)\n",
    "    df['Podcast_Name_Encoded'] = df['Podcast_Name_Encoded'].fillna(podcast_mean_listening.mean())\n",
    "    df.drop('Podcast_Name', axis=1, inplace=True)\n",
    "\n",
    "# Text Features from Episode_Title\n",
    "vectorizer = CountVectorizer(max_features=50, stop_words='english')\n",
    "vectorizer.fit(train_df['Episode_Title'])  # Fit on training data\n",
    "for df in [train_df, test_df]:\n",
    "    df['Title_Length'] = df['Episode_Title'].apply(lambda x: len(x.split()))\n",
    "    keywords = ['interview', 'exclusive', 'special', 'guest']\n",
    "    for kw in keywords:\n",
    "        df[f'Title_Has_{kw}'] = df['Episode_Title'].str.contains(kw, case=False, na=False).astype(int)\n",
    "    title_bow = vectorizer.transform(df['Episode_Title'])\n",
    "    bow_df = pd.DataFrame(title_bow.toarray(), \n",
    "                          columns=[f'Title_BOW_{f}' for f in vectorizer.get_feature_names_out()],\n",
    "                          index=df.index)\n",
    "    df.drop('Episode_Title', axis=1, inplace=True)\n",
    "    df = pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "# Is_Weekend from encoded Publication_Day\n",
    "for df in [train_df, test_df]:\n",
    "    weekend_cols = [col for col in df.columns if 'Publication_Day_Saturday' in col or 'Publication_Day_Sunday' in col]\n",
    "    df['Is_Weekend'] = df[weekend_cols].sum(axis=1).astype(int) if weekend_cols else 0\n",
    "\n",
    "# Interaction Features\n",
    "for df in [train_df, test_df]:\n",
    "    df['Combined_Popularity'] = (df['Host_Popularity_percentage'] + df['Guest_Popularity_percentage']) / 2\n",
    "    df['Ad_Density'] = df['Number_of_Ads'] / df['Episode_Length_minutes'].replace(0, 1)\n",
    "\n",
    "# Scaling Numerical Features\n",
    "num_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "            'Guest_Popularity_percentage', 'Number_of_Ads', \n",
    "            'Podcast_Name_Encoded', 'Title_Length', \n",
    "            'Combined_Popularity', 'Ad_Density']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[num_cols])  # Fit on engineered training data\n",
    "train_df[num_cols] = scaler.transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "# Debugging: Inspect test data features\n",
    "print(\"Test data feature stats after engineering:\\n\", test_df.drop('id', axis=1).describe())\n",
    "print(\"Missing values in test data:\\n\", test_df.isna().sum())\n",
    "\n",
    "# Ensure test data has same columns as training data\n",
    "engineered_train_df = pd.read_csv('engineered_podcast_data.csv')\n",
    "train_cols = engineered_train_df.drop('Listening_Time_minutes', axis=1).columns\n",
    "test_cols = test_df.columns\n",
    "missing_cols = [col for col in train_cols if col not in test_cols and col != 'id']\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  # Add missing columns with zeros\n",
    "extra_cols = [col for col in test_cols if col not in train_cols and col != 'id']\n",
    "test_df = test_df.drop(extra_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reorder columns to match training data\n",
    "test_df = test_df[train_cols]\n",
    "\n",
    "# Debugging: Compare columns\n",
    "print(\"Training columns:\", train_cols.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())\n",
    "\n",
    "# 4. Load the Model\n",
    "model = joblib.load('LinearRegression_model.pkl')\n",
    "print(\"Loaded Linear Regression model.\")\n",
    "print(\"Model coefficients:\\n\", model.coef_)\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "\n",
    "test_df['id'] = ids\n",
    "# 5. Make Predictions\n",
    "test_ids = test_df['id']\n",
    "test_features = test_df.drop('id', axis=1)\n",
    "predictions = model.predict(test_features)\n",
    "print(\"Prediction stats:\\n\", pd.Series(predictions).describe())\n",
    "\n",
    "# 6. Save Output\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Listening_Time_minutes': predictions\n",
    "})\n",
    "output_df['Listening_Time_minutes'] = output_df['Listening_Time_minutes'].clip(lower=0, upper=119.97)  # Clip to training range\n",
    "output_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions saved as 'predictions.csv'\")\n",
    "print(\"Output shape:\", output_df.shape)\n",
    "print(\"Sample predictions:\\n\", output_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
